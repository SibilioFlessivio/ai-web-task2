{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "schema = Schema(title_ix=TEXT(stored=True), content_ix=TEXT, headers_ix=TEXT, url_ix=TEXT(stored=True))\n",
    "\n",
    "ix = create_in(\"indexdir\", schema)\n",
    "writer = ix.writer()\n",
    "\n",
    "prefix = 'https://vm009.rz.uos.de/crawl/'\n",
    "start_url = prefix+'index.html'\n",
    "agenda = [start_url]\n",
    "visited_recently = []\n",
    "headers_v = []\n",
    "\n",
    "\n",
    "while agenda:\n",
    "    url = agenda.pop()\n",
    "    if url not in visited_recently:\n",
    "        visited_recently.append(url)\n",
    "        r = requests.get(url)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "            for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                headers_v.append(header.get_text())\n",
    "\n",
    "            content = soup.get_text()\n",
    "            writer.add_document(title_ix = soup.title.get_text(strip=True) if soup.title else \"No Title\", content_ix=soup.get_text(), headers_ix=' '.join(headers_v), url_ix=url)\n",
    "            \n",
    "            headers_v = []\n",
    "            \n",
    "            \n",
    "            for link in soup.find_all('a'):\n",
    "                \n",
    "                href = link.get('href')\n",
    "                # if the href is a relative link, we need to join it with the base url. If the href is an absolut link the function urljoin will return the href as it is.\n",
    "                full_url = urljoin(url, href)\n",
    "\n",
    "                # we dont want to scrape other weppages than the ones that start with the prefix\n",
    "                if full_url.startswith(prefix):\n",
    "                        agenda.append(full_url)\n",
    "writer.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'title_ix': 'Page 6', 'url_ix': 'https://vm009.rz.uos.de/crawl/page6.html'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "with ix.searcher() as searcher:\n",
    "    # find entries with the words 'first' AND 'last'\n",
    "    query = QueryParser(\"content_ix\", ix.schema).parse(\"pixels\")\n",
    "    results = searcher.search(query)\n",
    "\n",
    "    # print all results\n",
    "    for r in results:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITEL:  Page 6\n",
      "Author: ChatGPT-4\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Page 6\n",
      "\n",
      "\n",
      "This is Page 6\n",
      "\n",
      "    In a world of pixels and bytes so torn,\n",
      "Lived a quirky platypus and a geeky unicorn.\n",
      "Platypus coded, with fins so deft,\n",
      "While unicorn debugged, with every breath left.\n",
      "\n",
      "\"Did you know,\" said Plat, with a glint in his eye,\n",
      "\"That I'm a mammal that lays eggs, oh my!\"\n",
      "Unicorn chuckled, \"And I've a horn of might,\n",
      "But can't run basic scripts, try as I might.\"\n",
      "\n",
      "Together they ventured, in realms of code and lore,\n",
      "Fixing glitches, and chasing bugs galore.\n",
      "For in this nerdy tale, so uniquely spun,\n",
      "A platypus and unicorn made coding fun!\n",
      "    \n",
      "Author: ChatGPT-4\n",
      "Page 2\n",
      "Page 3\n",
      "Page 7\n",
      "Home Page\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uni_start = 'https://www.uni-osnabrueck.de/'\n",
    "r = requests.get('https://vm009.rz.uos.de/crawl/page6.html')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(\"TITEL: \",soup.title.get_text())\n",
    "for text in soup.find_all('p'):\n",
    "    print(text.get_text())\n",
    "print('--------------------------------------')\n",
    "print(soup.get_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
