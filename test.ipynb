{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "schema = Schema(title_ix=TEXT(stored=True), content_ix=TEXT, headers_ix=TEXT, url_ix=TEXT(stored=True))\n",
    "\n",
    "ix = create_in(\"indexdir\", schema)\n",
    "writer = ix.writer()\n",
    "\n",
    "prefix = 'https://vm009.rz.uos.de/crawl/'\n",
    "start_url = prefix+'index.html'\n",
    "agenda = [start_url]\n",
    "visited_recently = []\n",
    "headers_v = []\n",
    "\n",
    "\n",
    "while agenda:\n",
    "    url = agenda.pop()\n",
    "    if url not in visited_recently:\n",
    "        visited_recently.append(url)\n",
    "        r = requests.get(url)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "            for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                headers_v.append(header.get_text())\n",
    "\n",
    "            content = soup.get_text()\n",
    "            writer.add_document(title_ix = soup.title.get_text(strip=True) if soup.title else \"No Title\", content_ix=soup.get_text(), headers_ix=' '.join(headers_v), url_ix=url)\n",
    "            \n",
    "            headers_v = []\n",
    "            \n",
    "            \n",
    "            for link in soup.find_all('a'):\n",
    "                \n",
    "                href = link.get('href')\n",
    "                # if the href is a relative link, we need to join it with the base url. If the href is an absolut link the function urljoin will return the href as it is.\n",
    "                full_url = urljoin(url, href)\n",
    "\n",
    "                # we dont want to scrape other weppages than the ones that start with the prefix\n",
    "                if full_url.startswith(prefix):\n",
    "                        agenda.append(full_url)\n",
    "writer.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'title_ix': 'Page 6', 'url_ix': 'https://vm009.rz.uos.de/crawl/page6.html'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "with ix.searcher() as searcher:\n",
    "    # find entries with the words 'first' AND 'last'\n",
    "    query = QueryParser(\"content_ix\", ix.schema).parse(\"pixels\")\n",
    "    results = searcher.search(query)\n",
    "\n",
    "    # print all results\n",
    "    for r in results:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://vm009.rz.uos.de/crawl/index.html', 'https://vm009.rz.uos.de/crawl/page3.html', 'https://vm009.rz.uos.de/crawl/page6.html', 'https://vm009.rz.uos.de/crawl/page7.html', 'https://vm009.rz.uos.de/crawl/page2.html', 'https://vm009.rz.uos.de/crawl/page5.html', 'https://vm009.rz.uos.de/crawl/page1.html', 'https://vm009.rz.uos.de/crawl/page4.html']\n"
     ]
    }
   ],
   "source": [
    "print(visited_recently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "\n",
    "# Here, the structure of index entires is defined. You can add more fields with metadata, computed values etc.,\n",
    "# and use them for searching and ranking. \n",
    "# We only use a title and a text.\n",
    "#\n",
    "# The \"stored\" attribute is used for all parts that we want to be able to fully retrieve from the index\n",
    "#\n",
    "schema = Schema(title=TEXT(stored=True), content=TEXT, headers=TEXT, url=TEXT(stored=True))\n",
    "\n",
    "# Create an index in the directory indexdr (the directory must already exist!)\n",
    "ix = create_in(\"indexdir\", schema)\n",
    "writer = ix.writer()\n",
    "\n",
    "# now let's add some texts (=documents)\n",
    "writer.add_document(title=u\"First document\", content=u\"This is the first document we've added!\")\n",
    "writer.add_document(title=u\"Second document\", content=u\"The second one is even more interesting!\")\n",
    "writer.add_document(title=u\"Songtext\", content=u\"Music was my first love and it will be the last\")\n",
    "\n",
    "# write the index to the disk\n",
    "writer.commit()\n",
    "\n",
    "# Retrieving data\n",
    "from whoosh.qparser import QueryParser\n",
    "with ix.searcher() as searcher:\n",
    "    # find entries with the words 'first' AND 'last'\n",
    "    query = QueryParser(\"content\", ix.schema).parse(\"first last\")\n",
    "    results = searcher.search(query)\n",
    "\n",
    "    # print all results\n",
    "    for r in results:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITEL:  Page 6\n",
      "Author: ChatGPT-4\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Page 6\n",
      "\n",
      "\n",
      "This is Page 6\n",
      "\n",
      "    In a world of pixels and bytes so torn,\n",
      "Lived a quirky platypus and a geeky unicorn.\n",
      "Platypus coded, with fins so deft,\n",
      "While unicorn debugged, with every breath left.\n",
      "\n",
      "\"Did you know,\" said Plat, with a glint in his eye,\n",
      "\"That I'm a mammal that lays eggs, oh my!\"\n",
      "Unicorn chuckled, \"And I've a horn of might,\n",
      "But can't run basic scripts, try as I might.\"\n",
      "\n",
      "Together they ventured, in realms of code and lore,\n",
      "Fixing glitches, and chasing bugs galore.\n",
      "For in this nerdy tale, so uniquely spun,\n",
      "A platypus and unicorn made coding fun!\n",
      "    \n",
      "Author: ChatGPT-4\n",
      "Page 2\n",
      "Page 3\n",
      "Page 7\n",
      "Home Page\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uni_start = 'https://www.uni-osnabrueck.de/'\n",
    "r = requests.get('https://vm009.rz.uos.de/crawl/page6.html')\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(\"TITEL: \",soup.title.get_text())\n",
    "for text in soup.find_all('p'):\n",
    "    print(text.get_text())\n",
    "print('--------------------------------------')\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyIndexError",
     "evalue": "Index 'MAIN' does not exist in FileStorage('indexdir')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyIndexError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwhoosh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_dir\n\u001b[1;32m----> 2\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[43mopen_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindexdir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m ix\u001b[38;5;241m.\u001b[39munlock()\n",
      "File \u001b[1;32mc:\\Users\\auris\\anaconda3\\lib\\site-packages\\whoosh\\index.py:123\u001b[0m, in \u001b[0;36mopen_dir\u001b[1;34m(dirname, indexname, readonly, schema)\u001b[0m\n\u001b[0;32m    121\u001b[0m     indexname \u001b[38;5;241m=\u001b[39m _DEF_INDEX_NAME\n\u001b[0;32m    122\u001b[0m storage \u001b[38;5;241m=\u001b[39m FileStorage(dirname, readonly\u001b[38;5;241m=\u001b[39mreadonly)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\auris\\anaconda3\\lib\\site-packages\\whoosh\\index.py:421\u001b[0m, in \u001b[0;36mFileIndex.__init__\u001b[1;34m(self, storage, schema, indexname)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexname \u001b[38;5;241m=\u001b[39m indexname\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Try reading the TOC to see if it's possible\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[43mTOC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\auris\\anaconda3\\lib\\site-packages\\whoosh\\index.py:618\u001b[0m, in \u001b[0;36mTOC.read\u001b[1;34m(cls, storage, indexname, gen, schema)\u001b[0m\n\u001b[0;32m    616\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_latest_generation(storage, indexname)\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gen \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 618\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EmptyIndexError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not exist in \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m                               \u001b[38;5;241m%\u001b[39m (indexname, storage))\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m# Read the content of this index from the .toc file.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m tocfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_filename(indexname, gen)\n",
      "\u001b[1;31mEmptyIndexError\u001b[0m: Index 'MAIN' does not exist in FileStorage('indexdir')"
     ]
    }
   ],
   "source": [
    "from whoosh.index import open_dir\n",
    "ix = open_dir(\"indexdir\")\n",
    "\n",
    "ix.unlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
